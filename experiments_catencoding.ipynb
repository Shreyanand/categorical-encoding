{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/app-root/lib/python3.6/site-packages (0.9.1)\n",
      "Requirement already satisfied: dirty_cat in /opt/app-root/lib/python3.6/site-packages (0.0.5)\n",
      "Requirement already satisfied: category_encoders in /opt/app-root/lib/python3.6/site-packages (2.1.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/app-root/lib/python3.6/site-packages (from fasttext) (2.4.3)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.6/site-packages (from fasttext) (1.18.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/app-root/lib/python3.6/site-packages (from fasttext) (44.0.0)\n",
      "Requirement already satisfied: scipy in /opt/app-root/lib/python3.6/site-packages (from dirty_cat) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/app-root/lib/python3.6/site-packages (from dirty_cat) (0.22.1)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.6/site-packages (from dirty_cat) (2.22.0)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /opt/app-root/lib/python3.6/site-packages (from category_encoders) (0.11.1)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /opt/app-root/lib/python3.6/site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /opt/app-root/lib/python3.6/site-packages (from category_encoders) (0.23.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/app-root/lib/python3.6/site-packages (from scikit-learn>=0.20->dirty_cat) (0.14.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/app-root/lib/python3.6/site-packages (from requests->dirty_cat) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/app-root/lib/python3.6/site-packages (from requests->dirty_cat) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.6/site-packages (from requests->dirty_cat) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/app-root/lib/python3.6/site-packages (from requests->dirty_cat) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/app-root/lib/python3.6/site-packages (from patsy>=0.4.1->category_encoders) (1.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/app-root/lib/python3.6/site-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/app-root/lib/python3.6/site-packages (from pandas>=0.21.1->category_encoders) (2019.3)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting lightgbm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/9d/ddcb2f43aca194987f1a99e27edf41cf9bc39ea750c3371c2a62698c509a/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 437kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xgboost\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/91/551d37ba472bcbd70a25e667acc65a18a9d053657b13afcf0f87aa24d7bb/xgboost-1.0.2-py3-none-manylinux1_x86_64.whl (109.7MB)\n",
      "\u001b[K     |████████████████████████████████| 109.8MB 1.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/app-root/lib/python3.6/site-packages (from lightgbm) (1.18.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/app-root/lib/python3.6/site-packages (from lightgbm) (0.22.1)\n",
      "Requirement already satisfied: scipy in /opt/app-root/lib/python3.6/site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/app-root/lib/python3.6/site-packages (from scikit-learn->lightgbm) (0.14.1)\n",
      "Installing collected packages: lightgbm, xgboost\n",
      "Successfully installed lightgbm-2.3.1 xgboost-1.0.2\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext dirty_cat category_encoders\n",
    "!pip install lightgbm xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember only to use unsupervised methods\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self, filename, data_name, header='infer'):\n",
    "        self.data_name = data_name\n",
    "        self.data = pd.read_csv(filename, header=header)\n",
    "        self.clean()\n",
    "        self.X = self.y = None\n",
    "        self.X_train = self.X_test = self.y_train = self.y_test = None\n",
    "        \n",
    "    \n",
    "    def clean(self):\n",
    "        if self.data_name == 'kaggle_cat':\n",
    "            self.data = self.data.drop('id', axis=1)\n",
    "    \n",
    "    def get_input_target(self, supervised=True):\n",
    "        dataset = self.data.values\n",
    "        if supervised:\n",
    "            self.X = dataset[:, :-1].astype(str)\n",
    "            self.y = dataset[:,-1]\n",
    "            self.y.reshape((len(self.y), 1))\n",
    "        else:\n",
    "            self.X = dataset\n",
    "             \n",
    "    \n",
    "    def test_train_split(self, X, y, test_size=0.33, random_state=1):\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "            sss.get_n_splits(X, y)\n",
    "            for train_index, test_index in sss.split(X, y):\n",
    "                #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                self.X_train, self.X_test = X[train_index], X[test_index]\n",
    "                self.y_train, self.y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "#https://datascience.stackexchange.com/questions/39317/difference-between-ordinalencoder-and-labelencoder\n",
    "#check this out for ordinal vs label\n",
    "\n",
    "class encoding():\n",
    "    ## encode inputs: nothing -> X, y\n",
    "    ## encode target: (default label)\n",
    "    ## self.data\n",
    "    def __init__(self):\n",
    "        #self.data = data\n",
    "        self.encoder = None\n",
    "    \n",
    "class label(encoding):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #super().__init__(data)\n",
    "        self.encoder = [] #Save encoders in the object for later use\n",
    "        self.X_train = self.X_test = None\n",
    "        \n",
    "    def encode_inputs(self, X, mode):\n",
    "        X_enc = []\n",
    "        for e, col in enumerate(X.T):\n",
    "            if mode == \"train\":\n",
    "                #self.encoder = [LabelEncoder() for i in range(X.shape[1])]\n",
    "                encoder_col = LabelEncoder()\n",
    "                encoder_col.fit(list(col) + ['Unk']) #handle unkown labels for test\n",
    "                x_enc = encoder_col.transform(col)\n",
    "                self.encoder.append(encoder_col)\n",
    "                X_enc.append(x_enc)\n",
    "            else:\n",
    "                new_col = list(col)\n",
    "                for unique_item in np.unique(col):\n",
    "                    if unique_item not in self.encoder[e].classes_:\n",
    "                        new_col = ['Unk' if x==unique_item else x for x in new_col]\n",
    "                X_enc.append(self.encoder[e].transform(new_col))\n",
    "        return np.vstack(X_enc).T  \n",
    "    \n",
    "class ordinal(encoding):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #super().__init__(data)\n",
    "        self.encoder = OrdinalEncoder()\n",
    "        self.X_train = self.X_test = None\n",
    "        \n",
    "    def encode_inputs(self, X):\n",
    "        return self.encoder.fit_transform(X)\n",
    "    \n",
    "class OHE(encoding):\n",
    "    \n",
    "    def __init__(self, sparse=True):\n",
    "        #super().__init__(data)\n",
    "        self.encoder = OneHotEncoder(sparse=sparse, handle_unknown=\"ignore\")\n",
    "        self.X_train = self.X_test = None\n",
    "        \n",
    "    def encode_inputs(self, X, compression, mode):\n",
    "        if compression == None:\n",
    "            if mode == \"train\":\n",
    "                return self.encoder.fit_transform(X)\n",
    "            else:\n",
    "                return self.encoder.transform(X)\n",
    "        if compression == \"SVD\":\n",
    "            if mode == \"train\":\n",
    "                return self.encoder.fit_transform(X)\n",
    "            else:\n",
    "                return self.encoder.transform(X)\n",
    "            \n",
    "    \n",
    "#Why should data be in parent class? do we need it?\n",
    "\n",
    "# class target(encoding)\n",
    "\n",
    "# class EE(encoding)\n",
    "\n",
    "# class autoencoder(encoding)\n",
    "\n",
    "# 1) One hot encoding + Dimensionality reduction (PCA, SVD, autoencoder)\n",
    "\n",
    "# 2) Train model, get results\n",
    "\n",
    "# 3) Converting to word embedding problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_train = dataloader('data/kaggle_cat_train.csv', \"kaggle_cat\")\n",
    "kc_train.get_input_target()\n",
    "kc_test = dataloader('data/kaggle_cat_train.csv', \"kaggle_cat\")\n",
    "kc_test.get_input_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kag_label = label()\n",
    "kag_label.X_train = kag_label.encode_inputs(kc_train.X, mode=\"train\")\n",
    "kag_label.X_test = kag_label.encode_inputs(kc_test.X, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encode_inputs() missing 1 required positional argument: 'compression'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a8e5ee801384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkag_ohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkag_ohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkag_ohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkag_ohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkag_ohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkc_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: encode_inputs() missing 1 required positional argument: 'compression'"
     ]
    }
   ],
   "source": [
    "kag_ohe = OHE(sparse=True)\n",
    "kag_ohe.X_train = kag_ohe.encode_inputs(kc_train.X, mode=\"train\")\n",
    "kag_ohe.X_test = kag_ohe.encode_inputs(kc_test.X, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = dataloader('data/breast_cancer.csv', \"breast_cancer\", header=None)\n",
    "bc.get_input_target()\n",
    "bc.test_train_split(bc.X, bc.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_label = label()\n",
    "bc_label.X_train = bc_label.encode_inputs(bc.X_train, mode=\"train\")\n",
    "bc_label.X_test = bc_label.encode_inputs(bc.X_test, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_ohe = OHE(sparse=True)\n",
    "bc_ohe.X_train = kag_ohe.encode_inputs(kc_train.X, mode=\"train\")\n",
    "bc_ohe.X_test = kag_ohe.encode_inputs(kc_test.X, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = dataloader('data/Insights/insights.csv', \"insights\")\n",
    "insights.get_input_target(supervised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_label = label()\n",
    "insights.X_enc = ins_label.encode_inputs(insights.X.astype(str), mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_ohe = OHE(sparse=True)\n",
    "ins_ohe.X_enc = ins_ohe.encode_inputs(insights.X.astype(str), mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model\n",
    "#Think of how to structure this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (191, 9) (191, 1)\n",
      "Test (95, 9) (95, 1)\n"
     ]
    }
   ],
   "source": [
    "# load and summarize the dataset\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "\t# load the dataset as a pandas DataFrame\n",
    "\tdata = read_csv(filename, header=None)\n",
    "\t# retrieve numpy array\n",
    "\tdataset = data.values\n",
    "\t# split into input (X) and output (y) variables\n",
    "\tX = dataset[:, :-1]\n",
    "\ty = dataset[:,-1]\n",
    "\t# format all fields as string\n",
    "\tX = X.astype(str)\n",
    "\t# reshape target to be a 2d array\n",
    "\ty = y.reshape((len(y), 1))\n",
    "\treturn X, y\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('data/breast_cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def prepare_inputs(X_train, X_test):\n",
    "\toe = OrdinalEncoder()\n",
    "\toe.fit(X_train)\n",
    "\tX_train_enc = oe.transform(X_train)\n",
    "\tX_test_enc = oe.transform(X_test)\n",
    "\treturn X_train_enc, X_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\tle.fit(y_train)\n",
    "\ty_train_enc = le.transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\treturn y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target\n",
    "def prepare_targets_ordinal(y_train, y_test):\n",
    "\tle = OrdinalEncoder()\n",
    "\tle.fit(y_train)\n",
    "\ty_train_enc = le.transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\treturn y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "#y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "y_train_enc, y_test_enc = prepare_targets_ordinal(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Credits: https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.58\n"
     ]
    }
   ],
   "source": [
    "# define the  model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train_enc.shape[1], activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train_enc, y_train_enc, epochs=100, batch_size=16, verbose=0)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test_enc, y_test_enc, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.53\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_enc, y_train_enc, epochs=100, batch_size=16, verbose=0)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test_enc, y_test_enc, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/opt/app-root/lib/python3.6/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Code from https://github.com/pcerda/string_categorical_encoders\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, \\\n",
    "    LabelEncoder\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation, NMF, \\\n",
    "    TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import murmurhash3_32, check_random_state\n",
    "\n",
    "from fasttext import load_model\n",
    "import category_encoders as cat_enc\n",
    "from dirty_cat import SimilarityEncoder, TargetEncoder\n",
    "from dirty_cat.similarity_encoder import get_kmeans_prototypes\n",
    "\n",
    "import gamma_poisson_factorization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "#from sklearn_extra.kernel_methods import EigenProRegressor, EigenProClassifier\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#from get_data import Data, get_data_path\n",
    "#from constants import sample_seed, shuffle_seed, clf_seed\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# CE_HOME = os.environ.get('CE_HOME')\n",
    "# sys.path.append(os.path.abspath(os.path.join(\n",
    "#     CE_HOME, 'python', 'categorical_encoding')))\n",
    "# from get_data import get_data_path\n",
    "\n",
    "\n",
    "class OneHotEncoderRemoveOne(OneHotEncoder):\n",
    "    def __init__(self, n_values=None, categorical_features=None,\n",
    "                 categories='auto', sparse=True, dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.sparse = sparse\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.n_values = n_values\n",
    "        self.categorical_features = categorical_features\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        Xout = super().transform(X)\n",
    "        return Xout[:, :-1]\n",
    "\n",
    "\n",
    "class NgramNaiveFisherKernel(SimilarityEncoder):\n",
    "    \"\"\"\n",
    "    Fisher kernel for a simple n-gram probability distribution\n",
    "\n",
    "    For the moment, the default implementation uses the most-frequent\n",
    "    prototypes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ngram_range=(2, 4),\n",
    "                 categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='ignore', hashing_dim=None, n_prototypes=None,\n",
    "                 random_state=None, n_jobs=None):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.ngram_range = ngram_range\n",
    "        self.dtype = np.float64\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.hashing_dim = hashing_dim\n",
    "        self.n_prototypes = n_prototypes\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        X = self._check_X(X)\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if ((self.hashing_dim is not None) and\n",
    "                (not isinstance(self.hashing_dim, int))):\n",
    "            raise ValueError(\"value '%r' was specified for hashing_dim, \"\n",
    "                             \"which has invalid type, expected None or \"\n",
    "                             \"int.\" % self.hashing_dim)\n",
    "\n",
    "        if self.categories not in ['auto', 'most_frequent', 'k-means']:\n",
    "            for cats in self.categories:\n",
    "                if not np.all(np.sort(cats) == np.array(cats)):\n",
    "                    raise ValueError(\"Unsorted categories are not yet \"\n",
    "                                     \"supported\")\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.categories_ = list()\n",
    "        self.random_state_ = check_random_state(self.random_state)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                self.categories_.append(np.unique(Xi))\n",
    "            elif self.categories == 'most_frequent':\n",
    "                self.categories_.append(self.get_most_frequent(Xi))\n",
    "            elif self.categories == 'k-means':\n",
    "                uniques, count = np.unique(Xi, return_counts=True)\n",
    "                self.categories_.append(\n",
    "                    get_kmeans_prototypes(uniques, self.n_prototypes,\n",
    "                                          sample_weight=count,\n",
    "                                          random_state=self.random_state_))\n",
    "            else:\n",
    "                if self.handle_unknown == 'error':\n",
    "                    valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                    if not np.all(valid_mask):\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                self.categories_.append(np.array(self.categories[i],\n",
    "                                                 dtype=object))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using specified encoding scheme.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : 2-d array, shape [n_samples, n_features_new]\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = self._check_X(X)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for i in range(n_features):\n",
    "            Xi = X[:, i]\n",
    "            valid_mask = np.in1d(Xi, self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "\n",
    "        min_n, max_n = self.ngram_range\n",
    "\n",
    "        total_length = sum(len(x) for x in self.categories_)\n",
    "        X_out = np.empty((len(X), total_length), dtype=self.dtype)\n",
    "        current_length = 0\n",
    "        for j, cats in enumerate(self.categories_):\n",
    "            encoded_Xj = self._ngram_presence_fisher_kernel(X[:, j], cats)\n",
    "            X_out[:, current_length: current_length + len(cats)] = encoded_Xj\n",
    "            current_length += len(cats)\n",
    "        return X_out\n",
    "\n",
    "    def _ngram_presence_fisher_kernel(self, strings, cats):\n",
    "        \"\"\" given to arrays of strings, returns the\n",
    "        encoding matrix of size\n",
    "        len(strings) x len(cats)\n",
    "        kernel fisher with p\n",
    "        where p is the presence vector\n",
    "        \"\"\"\n",
    "        unq_strings = np.unique(strings)\n",
    "        unq_cats, count_j = np.unique(cats, return_counts=True)\n",
    "        theta = count_j/sum(count_j)\n",
    "        theta_sum = theta.sum()\n",
    "        vectorizer = CountVectorizer(analyzer='char',\n",
    "                                     ngram_range=self.ngram_range)\n",
    "        Cj = vectorizer.fit_transform(unq_cats)\n",
    "        Ci = vectorizer.transform(unq_strings)\n",
    "        m = Cj.shape[1]\n",
    "        SE_dict = {}\n",
    "        for i, c_i in enumerate(Ci):\n",
    "            gamma = np.ones(m) * theta_sum\n",
    "            similarity = []\n",
    "            for j, c_j in enumerate(Cj):\n",
    "                indicator = (c_j != c_i).astype('float64')\n",
    "                gamma -= indicator * theta[j]\n",
    "                similarity.append(indicator)\n",
    "            gamma_inv = 1 / gamma\n",
    "            del gamma\n",
    "            similarity = (gamma_inv.sum() -\n",
    "                          sparse.vstack(similarity).multiply(gamma_inv\n",
    "                                                             ).sum(axis=1))\n",
    "            SE_dict[unq_strings[i]] = similarity.reshape(1, -1)\n",
    "        SE = np.empty((len(strings), len(cats)))\n",
    "        for i, s in enumerate(strings):\n",
    "            SE[i, :] = SE_dict[s]\n",
    "        return np.nan_to_num(SE)\n",
    "\n",
    "    def _ngram_presence_fisher_kernel2(self, strings, cats):\n",
    "        \"\"\" given to arrays of strings, returns the\n",
    "        encoding matrix of size\n",
    "        len(strings) x len(cats)\n",
    "        kernel fisher with p\n",
    "        where p is the presence vector\n",
    "        \"\"\"\n",
    "        unq_strings = np.unique(strings)\n",
    "        unq_cats, count_j = np.unique(cats, return_counts=True)\n",
    "        theta = count_j/sum(count_j)\n",
    "        vectorizer = CountVectorizer(analyzer='char',\n",
    "                                     ngram_range=self.ngram_range,\n",
    "                                     binary=True)\n",
    "        Cj = vectorizer.fit_transform(unq_cats)\n",
    "        Ci = vectorizer.transform(unq_strings)\n",
    "        m = Cj.shape[1]\n",
    "        SE_dict = {}\n",
    "        for i, p_i in enumerate(Ci):\n",
    "            gamma = np.zeros(m)\n",
    "            for j, p_j in enumerate(Cj):\n",
    "                gamma += (p_j == p_i).astype('float64') * theta[j]\n",
    "            similarity = []\n",
    "            for j, p_j in enumerate(Cj):\n",
    "                sim_j = (p_j == p_i).astype('float64') / gamma\n",
    "                similarity.append(sim_j.sum())\n",
    "            SE_dict[unq_strings[i]] = np.array(similarity)\n",
    "        SE = np.empty((len(strings), len(cats)))\n",
    "        for i, s in enumerate(strings):\n",
    "            SE[i, :] = SE_dict[s]\n",
    "        return np.nan_to_num(SE)\n",
    "\n",
    "\n",
    "class PretrainedFastText(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Category embedding using a fastText pretrained model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components, language='english'):\n",
    "        self.n_components = n_components\n",
    "        self.language = language\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        path_dict = dict(\n",
    "            english='crawl-300d-2M-subword.bin',\n",
    "            french='cc.fr.300.bin',\n",
    "            hungarian='cc.hu.300.bin')\n",
    "\n",
    "        if self.language not in path_dict.keys():\n",
    "            raise AttributeError(\n",
    "                'language %s has not been downloaded yet' % self.language)\n",
    "\n",
    "        self.ft_model = load_model(os.path.join(get_data_path(), 'fastText',\n",
    "                                                path_dict[self.language]))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.ravel()\n",
    "        unq_X, lookup = np.unique(X, return_inverse=True)\n",
    "        X_dict = dict()\n",
    "        for i, x in enumerate(unq_X):\n",
    "            if x.find('\\n') != -1:\n",
    "                unq_X[i] = ' '.join(x.split('\\n'))\n",
    "\n",
    "        for x in unq_X:\n",
    "            X_dict[x] = self.ft_model.get_sentence_vector(x)\n",
    "\n",
    "        X_out = np.empty((len(lookup), 300))\n",
    "        for x, x_out in zip(unq_X[lookup], X_out):\n",
    "            x_out[:] = X_dict[x]\n",
    "        return X_out\n",
    "\n",
    "\n",
    "class MinHashEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    minhash method applied to ngram decomposition of strings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components, ngram_range=(2, 4)):\n",
    "        self.ngram_range = ngram_range\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def get_unique_ngrams(self, string, ngram_range):\n",
    "        \"\"\"\n",
    "        Return a list of different n-grams in a string\n",
    "        \"\"\"\n",
    "        spaces = ' '  # * (n // 2 + n % 2)\n",
    "        string = spaces + \" \".join(string.lower().split()) + spaces\n",
    "        ngram_list = []\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            string_list = [string[i:] for i in range(n)]\n",
    "            ngram_list += list(set(zip(*string_list)))\n",
    "        return ngram_list\n",
    "\n",
    "    def minhash(self, string, n_components, ngram_range):\n",
    "        min_hashes = np.ones(n_components) * np.infty\n",
    "        grams = self.get_unique_ngrams(string, self.ngram_range)\n",
    "        if len(grams) == 0:\n",
    "            grams = self.get_unique_ngrams(' Na ', self.ngram_range)\n",
    "        for gram in grams:\n",
    "            hash_array = np.array([\n",
    "                murmurhash3_32(''.join(gram), seed=d, positive=True)\n",
    "                for d in range(n_components)])\n",
    "            min_hashes = np.minimum(min_hashes, hash_array)\n",
    "        return min_hashes/(2**32-1)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.hash_dict = {}\n",
    "        for i, x in enumerate(X):\n",
    "            if x not in self.hash_dict:\n",
    "                self.hash_dict[x] = self.minhash(\n",
    "                    x, n_components=self.n_components,\n",
    "                    ngram_range=self.ngram_range)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X_out = np.zeros((len(X), self.n_components))\n",
    "\n",
    "        for i, x in enumerate(X):\n",
    "            if x not in self.hash_dict:\n",
    "                self.hash_dict[x] = self.minhash(\n",
    "                    x, n_components=self.n_components,\n",
    "                    ngram_range=self.ngram_range)\n",
    "\n",
    "        for i, x in enumerate(X):\n",
    "            X_out[i, :] = self.hash_dict[x]\n",
    "\n",
    "        return X_out\n",
    "\n",
    "\n",
    "class AdHocIndependentPDF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, fisher_kernel=True, dtype=np.float64,\n",
    "                 ngram_range=(2, 4)):\n",
    "        self.ngram_range = ngram_range\n",
    "        self.count_vectorizer = CountVectorizer(\n",
    "             analyzer='char', ngram_range=self.ngram_range)\n",
    "        self.fisher_kernel = fisher_kernel\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.cats, self.count = np.unique(X, return_counts=True)\n",
    "        self.pD = (self.count_vectorizer.fit_transform(self.cats) > 0)\n",
    "        self.theta = self.count / sum(self.count)\n",
    "        self.n_features, self.n_vocab = self.pD.shape\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        unqX = np.unique(X)\n",
    "        pX = (self.count_vectorizer.transform(unqX) > 0)\n",
    "        d = len(self.cats)\n",
    "        encoder_dict = {}\n",
    "        for i, px in enumerate(pX):\n",
    "            beta = np.ones((1, self.n_vocab))\n",
    "            for j, pd in enumerate(self.pD):\n",
    "                beta -= (px != pd) * self.theta[j]\n",
    "            inv_beta = 1 / beta\n",
    "            inv_beta_trans = inv_beta.transpose()\n",
    "            sum_inv_beta = inv_beta.sum()\n",
    "            fisher_vector = np.ones((1, d)) * sum_inv_beta\n",
    "            for j, pd in enumerate(self.pD):\n",
    "                fisher_vector[0, j] -= (px != pd).dot(inv_beta_trans)\n",
    "            encoder_dict[unqX[i]] = fisher_vector\n",
    "        Xout = np.zeros((X.shape[0], d))\n",
    "        for i, x in enumerate(X):\n",
    "            Xout[i, :] = encoder_dict[x]\n",
    "        return np.nan_to_num(Xout).astype(self.dtype)\n",
    "\n",
    "\n",
    "class NgramsMultinomialMixture(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Fisher kernel w/r to the mixture of unigrams model (Nigam, 2000).\n",
    "    \"\"\"\n",
    "    # TODO: add stop_criterion; implement k-means for count-vector;\n",
    "    # implement version with poisson distribution; add online_method\n",
    "\n",
    "    def __init__(self, n_topics=10, max_iters=100, fisher_kernel=True,\n",
    "                 beta_init_type=None, max_mean_change_tol=1e-5,\n",
    "                 ngram_range=(2, 4)):\n",
    "        self.ngram_range = ngram_range\n",
    "        self.ngrams_count = CountVectorizer(\n",
    "             analyzer='char', ngram_range=self.ngram_range)\n",
    "        self.n_topics = n_topics  # parameter k\n",
    "        self.max_iters = max_iters\n",
    "        self.fisher_kernel = fisher_kernel\n",
    "        self.beta_init_type = beta_init_type\n",
    "        self.max_mean_change_tol = max_mean_change_tol\n",
    "\n",
    "    def _get_most_frequent(self, X):\n",
    "        unqX, count = np.unique(X, return_counts=True)\n",
    "        # assert self.n_topics <= len(unqX)\n",
    "        count_sort_ind = np.argsort(-count)\n",
    "        most_frequent_cats = unqX[count_sort_ind][:self.n_topics]\n",
    "        count_most_frequent = count[count_sort_ind][:self.n_topics]\n",
    "        return most_frequent_cats, count_most_frequent\n",
    "\n",
    "    def _max_mean_change(self, last_beta, beta):\n",
    "        max_mean_change = max(abs((last_beta - beta)).sum(axis=1))\n",
    "        return max_mean_change\n",
    "\n",
    "    def _e_step(self, D, unqD, X, unqX, theta, beta):\n",
    "        log_doc_topic_posterior_dict = {}\n",
    "        log_fisher_kernel_dict = {}\n",
    "        for m, d in enumerate(unqD):\n",
    "            log_P_z_theta = np.log(theta)\n",
    "            log_beta = np.log(beta)\n",
    "            log_P_d_zbeta = np.array(\n",
    "                [d.dot(log_beta[i, :])[0] - 1 for i in range(self.n_topics)])\n",
    "            log_P_dz_thetabeta = log_P_d_zbeta + log_P_z_theta\n",
    "            log_doc_topic_posterior_dict[unqX[m]] = (\n",
    "                log_P_dz_thetabeta - logsumexp(log_P_dz_thetabeta))\n",
    "            log_fisher_kernel_dict[unqX[m]] = (\n",
    "                log_P_d_zbeta - logsumexp(log_P_dz_thetabeta))\n",
    "\n",
    "        log_doc_topic_posterior = np.zeros((D.shape[0], self.n_topics))\n",
    "        log_fisher_kernel = np.zeros((D.shape[0], self.n_topics))\n",
    "        for m, x in enumerate(X):\n",
    "            log_doc_topic_posterior[m, :] = log_doc_topic_posterior_dict[x]\n",
    "            log_fisher_kernel[m, :] = log_fisher_kernel_dict[x]\n",
    "        return np.exp(log_doc_topic_posterior), np.exp(log_fisher_kernel)\n",
    "\n",
    "    def _m_step(self, D, _doc_topic_posterior):\n",
    "        aux = np.dot(_doc_topic_posterior.transpose(), D.toarray())\n",
    "        beta = np.divide(1 + aux,\n",
    "                         np.sum(aux, axis=1).reshape(-1, 1) + self.n_vocab)\n",
    "        theta = ((1 + np.sum(_doc_topic_posterior, axis=0).reshape(-1)) /\n",
    "                 (self.n_topics + self.n_samples))\n",
    "        return theta, beta\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        unqX = np.unique(X)\n",
    "        unqD = self.ngrams_count.fit_transform(unqX)\n",
    "        D = self.ngrams_count.transform(X)\n",
    "        self.vocabulary = self.ngrams_count.get_feature_names()\n",
    "        self.n_samples, self.n_vocab = D.shape\n",
    "        prototype_cats, protoype_counts = self._get_most_frequent(X)\n",
    "        self.theta_prior = protoype_counts / self.n_topics\n",
    "        protoD = self.ngrams_count.transform(prototype_cats).toarray() + 1e-5\n",
    "        if self.beta_init_type == 'most-frequent-categories':\n",
    "            self.beta_prior = protoD / protoD.sum(axis=1).reshape(-1, 1)\n",
    "        if self.beta_init_type == 'constant':\n",
    "            self.beta_prior = (np.ones(protoD.shape) /\n",
    "                               protoD.sum(axis=1).reshape(-1, 1))\n",
    "        if self.beta_init_type == 'random':\n",
    "            np.random.seed(seed=42)\n",
    "            aux = np.random.uniform(0, 1, protoD.shape) + 1e-5\n",
    "            self.beta_prior = aux / protoD.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "        theta, beta = self.theta_prior, self.beta_prior\n",
    "        _last_beta = np.zeros((self.n_topics, self.n_vocab))\n",
    "        for i in range(self.max_iters):\n",
    "            for i in range(0):\n",
    "                print(i)\n",
    "            _doc_topic_posterior, _ = self._e_step(D, unqD, X, unqX,\n",
    "                                                   theta, beta)\n",
    "            theta, beta = self._m_step(D, _doc_topic_posterior)\n",
    "            max_mean_change = self._max_mean_change(_last_beta, beta)\n",
    "            if max_mean_change < self.max_mean_change_tol:\n",
    "                print('final n_iters: %d' % i)\n",
    "                print(max_mean_change)\n",
    "                break\n",
    "            _last_beta = beta\n",
    "        self.theta, self.beta = theta, beta\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        unqX = np.unique(X)\n",
    "        unqD = self.ngrams_count.transform(unqX)\n",
    "        D = self.ngrams_count.transform(X)\n",
    "        if type(self.fisher_kernel) is not bool:\n",
    "            raise TypeError('fisher_kernel parameter must be boolean.')\n",
    "        if self.fisher_kernel is True:\n",
    "            _, Xout = self._e_step(D, unqD, X, unqX, self.theta, self.beta)\n",
    "        if self.fisher_kernel is False:\n",
    "            Xout, _ = self._e_step(D, unqD, X, unqX, self.theta, self.beta)\n",
    "        return Xout\n",
    "\n",
    "\n",
    "class AdHocNgramsMultinomialMixture(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Fisher kernel w/r to the mixture of unigrams model (Nigam, 2000).\n",
    "    The dimensionality of the embedding is set to the number of unique\n",
    "    categories in the training set and the count vector matrix is give\n",
    "    as initial gues for the parameter beta.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iters=10, fisher_kernel=True, ngram_range=(2, 4)):\n",
    "        self.ngram_range = ngram_range\n",
    "        self.ngrams_count = CountVectorizer(\n",
    "             analyzer='char', ngram_range=self.ngram_range)\n",
    "        self.n_iters = n_iters\n",
    "        self.fisher_kernel = fisher_kernel\n",
    "\n",
    "    def _e_step(self, D, unqD, X, unqX, theta, beta):\n",
    "        doc_topic_posterior_dict = {}\n",
    "        fisher_kernel_dict = {}\n",
    "        for m, d in enumerate(unqD):\n",
    "            P_z_theta = theta\n",
    "            beta = beta\n",
    "            P_d_zbeta = np.array(\n",
    "                [float(d.dot(beta[i, :].transpose()).toarray()) - 1\n",
    "                 for i in range(self.n_topics)])\n",
    "            P_dz_thetabeta = P_d_zbeta * P_z_theta\n",
    "            doc_topic_posterior_dict[unqX[m]] = (\n",
    "                P_dz_thetabeta / P_dz_thetabeta.sum(axis=0))\n",
    "            fisher_kernel_dict[unqX[m]] = (\n",
    "                P_d_zbeta / P_dz_thetabeta.sum(axis=0))\n",
    "\n",
    "        doc_topic_posterior = np.zeros((D.shape[0], self.n_topics))\n",
    "        fisher_kernel = np.zeros((D.shape[0], self.n_topics))\n",
    "        for m, x in enumerate(X):\n",
    "            doc_topic_posterior[m, :] = doc_topic_posterior_dict[x]\n",
    "            fisher_kernel[m, :] = fisher_kernel_dict[x]\n",
    "        return doc_topic_posterior, fisher_kernel\n",
    "\n",
    "    def _m_step(self, D, _doc_topic_posterior):\n",
    "        aux = np.dot(_doc_topic_posterior.transpose(), D.toarray())\n",
    "        beta = np.divide(1 + aux,\n",
    "                         np.sum(aux, axis=1).reshape(-1, 1) + self.n_vocab)\n",
    "        theta = ((1 + np.sum(_doc_topic_posterior, axis=0).reshape(-1)) /\n",
    "                 (self.n_topics + self.n_samples))\n",
    "        return theta, beta\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        unqX, self.theta_prior = np.unique(X, return_counts=True)\n",
    "        self.theta_prior = self.theta_prior/self.theta_prior.sum()\n",
    "        self.n_topics = len(unqX)\n",
    "        unqD = self.ngrams_count.fit_transform(unqX)\n",
    "        D = self.ngrams_count.transform(X)\n",
    "        self.n_samples, self.n_vocab = D.shape\n",
    "        self.beta_prior = sparse.csr_matrix(unqD.multiply(1/unqD.sum(axis=1)))\n",
    "        theta, beta = self.theta_prior, self.beta_prior\n",
    "        for i in range(self.n_iters):\n",
    "            _doc_topic_posterior, _ = self._e_step(D, unqD, X, unqX,\n",
    "                                                   theta, beta)\n",
    "            theta, beta = self._m_step(D, _doc_topic_posterior)\n",
    "        self.theta, self.beta = theta, beta\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        unqX = np.unique(X)\n",
    "        D = self.ngrams_count.transform(X)\n",
    "        unqD = self.ngrams_count.transform(unqX)\n",
    "        if type(self.fisher_kernel) is not bool:\n",
    "            raise TypeError('fisher_kernel parameter must be boolean.')\n",
    "        if self.fisher_kernel is True:\n",
    "            _, Xout = self._e_step(D, unqD, X, unqX, self.theta, self.beta)\n",
    "        if self.fisher_kernel is False:\n",
    "            Xout, _ = self._e_step(D, unqD, X, unqX, self.theta, self.beta)\n",
    "        return Xout\n",
    "\n",
    "\n",
    "class MDVEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, clf_type):\n",
    "        self.clf_type = clf_type\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.clf_type in ['regression']:\n",
    "            pass\n",
    "        if self.clf_type in ['binary', 'multiclass']:\n",
    "            self.classes_ = np.unique(y)\n",
    "            self.categories_ = np.unique(X)\n",
    "            self.class_dict = {c: (y == c) for c in self.classes_}\n",
    "            self.Exy = {x: [] for x in self.categories_}\n",
    "            X_dict = {x: (X == x) for x in self.categories_}\n",
    "            for x in self.categories_:\n",
    "                for j, c in enumerate(self.classes_):\n",
    "                    aux1 = X_dict[x]\n",
    "                    aux2 = self.class_dict[c]\n",
    "                    self.Exy[x].append(np.mean(aux1[aux2]))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.clf_type in ['regression']:\n",
    "            pass\n",
    "        if self.clf_type in ['binary', 'multiclass']:\n",
    "            Xout = np.zeros((len(X), len(self.classes_)))\n",
    "            for i, x in enumerate(X):\n",
    "                if x in self.Exy:\n",
    "                    Xout[i, :] = self.Exy[x]\n",
    "                else:\n",
    "                    Xout[i, :] = 0\n",
    "            return Xout\n",
    "\n",
    "\n",
    "def test_MDVEncoder():\n",
    "    X_train = np.array(\n",
    "        ['hola', 'oi', 'bonjour', 'hola', 'oi',\n",
    "         'hola', 'oi', 'oi', 'hola'])\n",
    "    y_train = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    X_test = np.array(['hola', 'bonjour', 'hola', 'oi', 'hello'])\n",
    "    ans = np.array([[2/4, 2/5],\n",
    "                    [0, 1/5],\n",
    "                    [2/4, 2/5],\n",
    "                    [2/4, 2/5],\n",
    "                    [0, 0]])\n",
    "    encoder = MDVEncoder(clf_type='binary-clf')\n",
    "    encoder.fit(X_train, y_train)\n",
    "    assert np.array_equal(encoder.transform(X_test), ans)\n",
    "\n",
    "\n",
    "class PasstroughEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, passthrough=True):\n",
    "        self.passthrough = passthrough\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder = FunctionTransformer(None, validate=True)\n",
    "        self.encoder.fit(X)\n",
    "        # self.columns = np.array(X.columns)\n",
    "        return self\n",
    "\n",
    "    # def get_feature_names(self):\n",
    "    #     return self.columns\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.encoder.transform(X)\n",
    "\n",
    "\n",
    "class ColumnEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 encoder_name,\n",
    "                 reduction_method=None,\n",
    "                 ngram_range=(2, 4),\n",
    "                 categories='auto',\n",
    "                 dtype=np.float64,\n",
    "                 handle_unknown='ignore',\n",
    "                 clf_type=None,\n",
    "                 n_components=None):\n",
    "        self.ngram_range = ngram_range\n",
    "        self.encoder_name = encoder_name\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.clf_type = clf_type\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.reduction_method = reduction_method\n",
    "        self.n_components = n_components\n",
    "        self.encoders_dict = {\n",
    "            'OneHotEncoder': OneHotEncoder(handle_unknown='ignore'),\n",
    "            'OneHotEncoder-1': OneHotEncoderRemoveOne(handle_unknown='ignore'),\n",
    "            'Categorical': None,\n",
    "            'OneHotEncoderDense': OneHotEncoder(\n",
    "                handle_unknown='ignore', sparse=False),\n",
    "            'OneHotEncoderDense-1': OneHotEncoderRemoveOne(\n",
    "                handle_unknown='ignore', sparse=False),\n",
    "            'SimilarityEncoder': SimilarityEncoder(\n",
    "                ngram_range=self.ngram_range, random_state=10),\n",
    "            'NgramNaiveFisherKernel': NgramNaiveFisherKernel(\n",
    "                ngram_range=self.ngram_range, random_state=10),\n",
    "            'ngrams_hot_vectorizer': [],\n",
    "            'NgramsCountVectorizer': CountVectorizer(\n",
    "                analyzer='char', ngram_range=self.ngram_range),\n",
    "            'NgramsTfIdfVectorizer': TfidfVectorizer(\n",
    "                analyzer='char', ngram_range=self.ngram_range,\n",
    "                smooth_idf=False),\n",
    "            'WordNgramsTfIdfVectorizer': TfidfVectorizer(\n",
    "                analyzer='word', ngram_range=(1, 1),\n",
    "                smooth_idf=False),\n",
    "            'TargetEncoder': TargetEncoder(\n",
    "                clf_type=self.clf_type, handle_unknown='ignore'),\n",
    "            'MDVEncoder': MDVEncoder(self.clf_type),\n",
    "            'BackwardDifferenceEncoder': cat_enc.BackwardDifferenceEncoder(),\n",
    "            'BinaryEncoder': cat_enc.BinaryEncoder(),\n",
    "            'HashingEncoder': cat_enc.HashingEncoder(),\n",
    "            'HelmertEncoder': cat_enc.HelmertEncoder(),\n",
    "            'SumEncoder': cat_enc.SumEncoder(),\n",
    "            'PolynomialEncoder': cat_enc.PolynomialEncoder(),\n",
    "            'BaseNEncoder': cat_enc.BaseNEncoder(),\n",
    "            'LeaveOneOutEncoder': cat_enc.LeaveOneOutEncoder(),\n",
    "            'NgramsLDA': Pipeline([\n",
    "                ('ngrams_count',\n",
    "                 CountVectorizer(\n",
    "                     analyzer='char', ngram_range=self.ngram_range)),\n",
    "                ('LDA', LatentDirichletAllocation(\n",
    "                    n_components=self.n_components, learning_method='batch'),)\n",
    "                ]),\n",
    "            'NMF': Pipeline([\n",
    "                ('ngrams_count',\n",
    "                 CountVectorizer(\n",
    "                     analyzer='char', ngram_range=self.ngram_range)),\n",
    "                ('NMF', NMF(\n",
    "                    n_components=self.n_components))\n",
    "                ]),\n",
    "            'WordNMF': Pipeline([\n",
    "                ('ngrams_count',\n",
    "                 CountVectorizer(\n",
    "                     analyzer='word', ngram_range=(1, 1))),\n",
    "                ('NMF', NMF(\n",
    "                    n_components=self.n_components))\n",
    "                ]),\n",
    "            'NgramsMultinomialMixture':\n",
    "                NgramsMultinomialMixture(\n",
    "                    n_topics=self.n_components, max_iters=10),\n",
    "            'AdHocNgramsMultinomialMixture':\n",
    "                AdHocNgramsMultinomialMixture(n_iters=0),\n",
    "            'AdHocIndependentPDF': AdHocIndependentPDF(),\n",
    "            'OnlineGammaPoissonFactorization':\n",
    "                gamma_poisson_factorization.OnlineGammaPoissonFactorization(\n",
    "                    n_topics=self.n_components, rho=.99, r=None,\n",
    "                    tol=1e-4, random_state=18, init='k-means++',\n",
    "                    ngram_range=self.ngram_range,\n",
    "                    rescale_W=True, max_iter_e_step=10),\n",
    "            'OnlineGammaPoissonFactorization2':\n",
    "                gamma_poisson_factorization.OnlineGammaPoissonFactorization(\n",
    "                    n_topics=self.n_components, r=.3, rho=None,\n",
    "                    batch_size=256,\n",
    "                    tol=1e-4, random_state=18, init='k-means++',\n",
    "                    ngram_range=self.ngram_range,\n",
    "                    rescale_W=True, max_iter_e_step=20),\n",
    "            'OnlineGammaPoissonFactorization3':\n",
    "                gamma_poisson_factorization.OnlineGammaPoissonFactorization(\n",
    "                    n_topics=self.n_components, r=.3, rho=None,\n",
    "                    batch_size=256,\n",
    "                    tol=1e-4, random_state=18, init='k-means',\n",
    "                    ngram_range=self.ngram_range,\n",
    "                    rescale_W=True, max_iter_e_step=20),\n",
    "            'OnlineGammaPoissonFactorization4':\n",
    "                gamma_poisson_factorization.OnlineGammaPoissonFactorization(\n",
    "                    n_topics=self.n_components, r=None, rho=.95,\n",
    "                    batch_size=256,\n",
    "                    tol=1e-4, random_state=18, init='k-means',\n",
    "                    ngram_range=self.ngram_range,\n",
    "                    rescale_W=True, max_iter_e_step=20),\n",
    "            'WordOnlineGammaPoissonFactorization':\n",
    "                gamma_poisson_factorization.OnlineGammaPoissonFactorization(\n",
    "                    n_topics=self.n_components, r=.3,\n",
    "                    tol=1e-4, random_state=18, init='k-means++',\n",
    "                    ngram_range=(1, 1), analizer='word',\n",
    "                    rescale_W=True, max_iter_e_step=10),\n",
    "            'OnlineGammaPoissonFactorization_fast':\n",
    "                gamma_poisson_factorization.OnlineGammaPoissonFactorization(\n",
    "                    n_topics=self.n_components, r=.3, ngram_range=(3, 3),\n",
    "                    max_iter=1, min_iter=1,\n",
    "                    tol=1e-4, random_state=18, init='k-means++',\n",
    "                    rescale_W=False),\n",
    "            'MinHashEncoder': MinHashEncoder(\n",
    "                n_components=self.n_components),\n",
    "            'PretrainedFastText':\n",
    "                PretrainedFastText(n_components=self.n_components),\n",
    "            'PretrainedFastText_fr':\n",
    "                PretrainedFastText(n_components=self.n_components,\n",
    "                                   language='french'),\n",
    "            'PretrainedFastText_hu':\n",
    "                PretrainedFastText(n_components=self.n_components,\n",
    "                                   language='hungarian'),\n",
    "            None: FunctionTransformer(None, validate=True),\n",
    "            'Passthrough': PasstroughEncoder(),\n",
    "            }\n",
    "        self.list_1D_array_methods = [\n",
    "            'NgramsCountVectorizer',\n",
    "            'NgramsTfIdfVectorizer',\n",
    "            'WordNgramsTfIdfVectorizer',\n",
    "            'ngrams_hot_vectorizer',\n",
    "            'NgramsLDA',\n",
    "            'NMF',\n",
    "            'WordNMF',\n",
    "            'NgramsMultinomialMixture',\n",
    "            'NgramsMultinomialMixtureKMeans2',\n",
    "            'AdHocNgramsMultinomialMixture',\n",
    "            'AdHocIndependentPDF',\n",
    "            'GammaPoissonFactorization',\n",
    "            'OnlineGammaPoissonFactorization',\n",
    "            'WordOnlineGammaPoissonFactorization',\n",
    "            'OnlineGammaPoissonFactorization2',\n",
    "            'OnlineGammaPoissonFactorization3',\n",
    "            'OnlineGammaPoissonFactorization4',\n",
    "            'OnlineGammaPoissonFactorization_fast',\n",
    "            'MinHashEncoder',\n",
    "            'MinMeanMinHashEncoder',\n",
    "            ]\n",
    "\n",
    "    def _get_most_frequent(self, X):\n",
    "        unqX, count = np.unique(X, return_counts=True)\n",
    "        if self.n_components <= len(unqX):\n",
    "            warnings.warn(\n",
    "                'Dimensionality reduction will not be applied because' +\n",
    "                'the encoding dimension is smaller than the required' +\n",
    "                'dimensionality: %d instead of %d' %\n",
    "                (X.shape[1], self.n_components))\n",
    "            return unqX.ravel()\n",
    "        else:\n",
    "            count_sort_ind = np.argsort(-count)\n",
    "            most_frequent_cats = unqX[count_sort_ind][:self.n_components]\n",
    "            return np.sort(most_frequent_cats)\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        try:\n",
    "            feature_names = self.encoder.get_feature_names()\n",
    "        except AttributeError:\n",
    "            feature_names = self.columns\n",
    "        return feature_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        assert X.values.ndim == 1\n",
    "        self.columns = X.name\n",
    "        X = X.values\n",
    "\n",
    "        if self.encoder_name not in self.encoders_dict:\n",
    "            template = (\"Encoder %s has not been implemented yet\")\n",
    "            raise ValueError(template % self.encoder_name)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoder_name == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        # if self.reduction_method == 'MostFrequentCategories':\n",
    "            # unq_cats = self._get_most_frequent(X)\n",
    "            # _X = []\n",
    "            # for x in X:\n",
    "            #     if x in unq_cats:\n",
    "            #         _X.append(x)\n",
    "            # X = np.array(_X)\n",
    "            # del _X\n",
    "\n",
    "        if self.categories != 'auto':\n",
    "            for cats in self.categories:\n",
    "                if not np.all(np.sort(cats) == np.array(cats)):\n",
    "                    raise ValueError(\"Unsorted categories are not yet \"\n",
    "                                     \"supported\")\n",
    "        self.le = LabelEncoder()\n",
    "\n",
    "        if self.categories == 'auto':\n",
    "            self.le.fit(X,)\n",
    "        else:\n",
    "            if self.handle_unknown == 'error':\n",
    "                valid_mask = np.in1d(X, self.categories)\n",
    "                if not np.all(valid_mask):\n",
    "                    msg = (\"Found unknown categories during fit\")\n",
    "                    raise ValueError(msg)\n",
    "            self.le.classes_ = np.array(self.categories)\n",
    "\n",
    "        self.categories_ = self.le.classes_\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        try:\n",
    "            self.n_features = X.shape[1]\n",
    "        except IndexError:\n",
    "            self.n_features = 1\n",
    "\n",
    "        if self.encoder_name in self.list_1D_array_methods:\n",
    "            assert self.n_features == 1\n",
    "            X = X.reshape(-1)\n",
    "        else:\n",
    "            X = X.reshape(n_samples, self.n_features)\n",
    "\n",
    "        if self.n_features > 1:\n",
    "            raise ValueError(\"Encoder does not support more than one feature.\")\n",
    "\n",
    "        self.encoder = self.encoders_dict[self.encoder_name]\n",
    "\n",
    "        if self.reduction_method == 'most_frequent':\n",
    "            assert self.n_features == 1\n",
    "            if len(np.unique(X)) <= self.n_components:\n",
    "                warnings.warn(\n",
    "                    'Dimensionality reduction will not be applied because ' +\n",
    "                    'the encoding dimension is smaller than the required ' +\n",
    "                    'dimensionality: %d instead of %d' %\n",
    "                    (len(np.unique(X)), self.n_components))\n",
    "                self.pipeline = Pipeline([\n",
    "                    ('encoder', self.encoder)\n",
    "                    ])\n",
    "            else:\n",
    "                self.encoder.categories = 'most_frequent'\n",
    "                self.encoder.n_prototypes = self.n_components\n",
    "                self.pipeline = Pipeline([\n",
    "                    ('encoder', self.encoder)\n",
    "                    ])\n",
    "        elif self.reduction_method == 'k-means':\n",
    "            assert 'SimilarityEncoder' in self.encoder_name\n",
    "            assert self.n_features == 1\n",
    "            if len(np.unique(X)) <= self.n_components:\n",
    "                warnings.warn(\n",
    "                    'Dimensionality reduction will not be applied because ' +\n",
    "                    'the encoding dimension is smaller than the required ' +\n",
    "                    'dimensionality: %d instead of %d' %\n",
    "                    (len(np.unique(X)), self.n_components))\n",
    "                self.pipeline = Pipeline([\n",
    "                    ('encoder', self.encoder)\n",
    "                    ])\n",
    "            else:\n",
    "                self.encoder.categories = 'k-means'\n",
    "                self.encoder.n_prototypes = self.n_components\n",
    "                self.pipeline = Pipeline([\n",
    "                    ('encoder', self.encoder)\n",
    "                    ])\n",
    "        elif self.reduction_method is None:\n",
    "            self.pipeline = Pipeline([\n",
    "                ('encoder', self.encoder)\n",
    "                ])\n",
    "        else:\n",
    "            self.pipeline = Pipeline([\n",
    "                ('encoder', self.encoder),\n",
    "                ('dimension_reduction',\n",
    "                 DimensionalityReduction(method_name=self.reduction_method,\n",
    "                                         n_components=self.n_components))\n",
    "                ])\n",
    "        # for MostFrequentCategories, change the fit method to consider only\n",
    "        # the selected categories\n",
    "        self.pipeline.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        if self.encoder_name in self.list_1D_array_methods:\n",
    "            pass\n",
    "        else:\n",
    "            X = X.values.reshape(n_samples, self.n_features)\n",
    "        Xout = self.pipeline.transform(X)\n",
    "        # if Xout.ndim == 1:\n",
    "        #     Xout.reshape(-1, 1)\n",
    "        return Xout\n",
    "\n",
    "\n",
    "class DimensionalityReduction(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method_name=None, n_components=None,\n",
    "                 column_names=None):\n",
    "        self.method_name = method_name\n",
    "        self.n_components = n_components\n",
    "        self.methods_dict = {\n",
    "            None: FunctionTransformer(None, accept_sparse=True, validate=True),\n",
    "            'GaussianRandomProjection': GaussianRandomProjection(\n",
    "                n_components=self.n_components, random_state=35),\n",
    "            'TruncatedSVD': TruncatedSVD(\n",
    "                n_components=self.n_components, random_state=35),\n",
    "            'most_frequent': 0,\n",
    "            'k-means': 0,\n",
    "            'PCA': PCA(n_components=self.n_components, random_state=87)\n",
    "            }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.method_name not in self.methods_dict:\n",
    "            template = (\"Dimensionality reduction method '%s' has not been \"\n",
    "                        \"implemented yet\")\n",
    "            raise ValueError(template % self.method_name)\n",
    "\n",
    "        self.method = self.methods_dict[self.method_name]\n",
    "        if self.n_components is not None:\n",
    "            if self.method_name is not None:\n",
    "                if X.shape[1] <= self.n_components:\n",
    "                    self.method = self.methods_dict[None]\n",
    "                    warnings.warn(\n",
    "                        'Dimensionality reduction will not be applied ' +\n",
    "                        'because the encoding dimension is smaller than ' +\n",
    "                        'the required dimensionality: %d instead of %d' %\n",
    "                        (X.shape[1], self.n_components))\n",
    "\n",
    "        self.method.fit(X)\n",
    "        self.n_features = 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xout = self.method.transform(X)\n",
    "        if Xout.ndim == 1:\n",
    "            return Xout.reshape(-1, 1)\n",
    "        else:\n",
    "            return Xout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember only to use unsupervised methods\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self, filename, data_name, header='infer'):\n",
    "        self.data_name = data_name\n",
    "        self.data = pd.read_csv(filename, header=header)\n",
    "        self.clean()\n",
    "        self.X = self.y = None\n",
    "        self.X_train = self.X_test = self.y_train = self.y_test = None\n",
    "        self.col_encoders = None\n",
    "    \n",
    "    def clean(self):\n",
    "        if self.data_name == 'kaggle_cat':\n",
    "            self.data = self.data.drop('id', axis=1)\n",
    "    \n",
    "    def get_input_target(self, supervised=True):\n",
    "        #dataset = self.data.values\n",
    "        if supervised:\n",
    "            self.X = self.data.iloc[:, :-1].astype(str)\n",
    "            self.y = self.data.iloc[:,-1]\n",
    "            #self.y.reshape((len(self.y), 1))\n",
    "        else:\n",
    "            self.X = self.data\n",
    "             \n",
    "    \n",
    "    def test_train_split(self, X, y, test_size=0.33, random_state=1):\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "            sss.get_n_splits(X, y)\n",
    "            for train_index, test_index in sss.split(X, y):\n",
    "                #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                self.X_train, self.X_test = X[train_index], X[test_index]\n",
    "                self.y_train, self.y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_train = dataloader('data/kaggle_cat_train.csv', \"kaggle_cat\")\n",
    "kc_train.get_input_target()\n",
    "kc_test = dataloader('data/kaggle_cat_train.csv', \"kaggle_cat\")\n",
    "kc_test.get_input_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = 'OneHotEncoderDense'\n",
    "kc_train.col_encoders = {col:  ColumnEncoder(encoder_name,\n",
    "                 reduction_method=None,\n",
    "                 ngram_range=(2, 4),\n",
    "                 categories='auto', #auto label binarizer categories\n",
    "                 dtype=np.float64,\n",
    "                 handle_unknown='ignore',\n",
    "                 clf_type=None,  # Requied for some methods\n",
    "                 n_components=None) # Dimensionality reduction, LDA etc. components\n",
    "    for col in kc_train.X.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_combine = []\n",
    "for col in kc_train.X:\n",
    "    kc_train.col_encoders[col].fit(kc_train.X[col])\n",
    "    col_combine.append(kc_train.col_encoders[col].transform(kc_train.X[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use column transformer here; see what's wrong here, memory leak\n",
    "col_combine = []\n",
    "cT = ColumnTransformer([(col, kc_train.col_encoders[col], col) for col in kc_train.X])\n",
    "cT.fit_transform(kc_train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### It seems they are using this encoding for only one special column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "requirements": {
   "aliases": {},
   "dev-packages": {},
   "packages": {
    "category-encoders": "*",
    "dirty_cat": "*",
    "fasttext": "*",
    "insights-core": "*",
    "keras": "*",
    "lightgbm": "*",
    "pandas": "*",
    "s3fs": "*",
    "scipy": "*",
    "sklearn": "*",
    "xgboost": "*"
   },
   "requires": {
    "python_version": "3.6"
   },
   "sources": [
    {
     "name": "pypi",
     "url": "https://pypi.org/simple",
     "verify_ssl": true
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
